{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8554467",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')  # Non-interactive backend\n",
    "from typing import List, Tuple\n",
    "import random\n",
    "from scipy import stats as scipy_stats\n",
    "\n",
    "# Add project root to path\n",
    "try:\n",
    "    project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), \"../../\"))\n",
    "except NameError:\n",
    "    project_root = os.path.abspath(\"../../\")\n",
    "\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "from utils.data.curating_stooq import curate_stooq_dir_5min\n",
    "from utils.data.jump_detection import detect_jumps_many\n",
    "from model.wavelet.wavelet import WaveletModel\n",
    "from plot_utils import save_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ad72a0",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def plot_profiles(X_windows, jumps_subset, output_dir, name):\n",
    "    \"\"\"\n",
    "    Plots average temporal profiles along PCA directions.\n",
    "    X-axis is time relative to jump (centered at 0).\n",
    "    \"\"\"\n",
    "    directions = [\"D1_reflexivity\", \"D2_mean_reversion\", \"D3_trend\"]\n",
    "    center = X_windows.shape[1] // 2\n",
    "    t_axis = np.arange(-center, center + 1)\n",
    "    \n",
    "    for dim in directions:\n",
    "        if dim not in jumps_subset.columns: continue\n",
    "        \n",
    "        scores = jumps_subset[dim].values\n",
    "        sorted_idx = np.argsort(scores)\n",
    "        X_sorted = X_windows[sorted_idx]\n",
    "        n = len(scores)\n",
    "        \n",
    "        # Quantiles to visualize (Low, Mid, High)\n",
    "        quantiles = [0, 0.1, 0.3, 0.5, 0.7, 0.9, 1.0]\n",
    "        \n",
    "        fig = go.Figure()\n",
    "        colors = px.colors.sequential.Viridis\n",
    "        \n",
    "        for i in range(len(quantiles)-1):\n",
    "            q_s, q_e = quantiles[i], quantiles[i+1]\n",
    "            idx_s, idx_e = int(q_s*n), int(q_e*n)\n",
    "            if idx_e <= idx_s: continue\n",
    "            \n",
    "            avg = np.mean(X_sorted[idx_s:idx_e], axis=0)\n",
    "            \n",
    "            color_idx = int(i / (len(quantiles)-1) * (len(colors)-1))\n",
    "            color = colors[color_idx]\n",
    "            \n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=t_axis, \n",
    "                y=avg, \n",
    "                mode='lines', \n",
    "                name=f\"Q {q_s}-{q_e}\",\n",
    "                line=dict(color=color, width=2)\n",
    "            ))\n",
    "            \n",
    "        fig.update_layout(\n",
    "            title=f\"Average Profiles along {dim} - {name}<br>(X-axis: Time relative to Jump)\",\n",
    "            xaxis_title=\"Time (steps)\",\n",
    "            yaxis_title=\"Normalized Return x(t)\",\n",
    "            template=\"plotly_white\",\n",
    "            hovermode=\"x unified\"\n",
    "        )\n",
    "        fig.add_vline(x=0, line_dash=\"dash\", line_color=\"red\", annotation_text=\"Jump\")\n",
    "        \n",
    "        out_path = os.path.join(output_dir, f\"{name}_profile_{dim}.html\")\n",
    "        fig.write_html(out_path)\n",
    "        print(f\"    Saved profile plot to {out_path}\")\n",
    "        save_plot(fig, f\"reproduce_hong_kong_profile_{dim}\", format='pdf')\n",
    "        \n",
    "        # Show in notebook\n",
    "        fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202ea473",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def run_analysis_for_subset(\n",
    "    dfs_subset: dict, \n",
    "    subset_name: str, \n",
    "    output_dir: str, \n",
    "    window_steps: int = 12\n",
    "):\n",
    "    print(f\"\\n=== Running Analysis for {subset_name} ({len(dfs_subset)} stocks) ===\")\n",
    "    \n",
    "    # 1. Filter Trading Hours (10:30 - 15:00 approx)\n",
    "    # Remove first/last 60 mins from each day\n",
    "    print(\"  Filtering trading hours...\")\n",
    "    filtered_dfs = {}\n",
    "    total_days = 0\n",
    "    for ticker, df in dfs_subset.items():\n",
    "        days = []\n",
    "        for date, day_df in df.groupby(df.index.date):\n",
    "            if len(day_df) <= 12: continue\n",
    "            day_df = day_df.sort_index()\n",
    "            \n",
    "            # For 5-min bars, 60 mins = 12 bars\n",
    "            start = day_df.index[0] + pd.Timedelta(minutes=60)\n",
    "            end = day_df.index[-1] - pd.Timedelta(minutes=60)\n",
    "            \n",
    "            mask = (day_df.index >= start) & (day_df.index <= end)\n",
    "            if mask.any():\n",
    "                days.append(day_df[mask])\n",
    "        \n",
    "        if days:\n",
    "            filtered_dfs[ticker] = pd.concat(days)\n",
    "            total_days += len(days)\n",
    "\n",
    "    if not filtered_dfs:\n",
    "        print(\"  No data remaining after filtering.\")\n",
    "        return\n",
    "\n",
    "    # 2. Detect Jumps\n",
    "    print(\"  Detecting jumps...\")\n",
    "    jumps_df = detect_jumps_many(filtered_dfs, threshold=4.0)\n",
    "    print(f\"  Detected {len(jumps_df)} total jumps.\")\n",
    "    \n",
    "    if len(jumps_df) < 50:\n",
    "        print(\"  Not enough jumps for robust PCA (need > 50).\")\n",
    "        return\n",
    "\n",
    "    # 3. Extract Windows\n",
    "    print(\"  Extracting windows...\")\n",
    "    windows = []\n",
    "    valid_indices = []\n",
    "    \n",
    "    for idx, row in jumps_df.iterrows():\n",
    "        ticker, ts = row[\"ticker\"], row[\"timestamp\"]\n",
    "        if ticker not in filtered_dfs: continue\n",
    "        df = filtered_dfs[ticker]\n",
    "        \n",
    "        if ts not in df.index: continue\n",
    "        loc = df.index.get_loc(ts)\n",
    "        \n",
    "        if loc - window_steps < 0 or loc + window_steps + 1 > len(df): continue\n",
    "        \n",
    "        subset = df.iloc[loc - window_steps : loc + window_steps + 1]\n",
    "        \n",
    "        # Normalization\n",
    "        norm = row[\"f\"] * row[\"sigma\"]\n",
    "        if norm == 0: norm = 1e-4\n",
    "            \n",
    "        r_window = subset[\"close\"].pct_change().fillna(0.0).values\n",
    "        x_profile = r_window / norm\n",
    "        \n",
    "        # Align Jump Direction (Paper convention: Jump > 0)\n",
    "        # Center is at index `window_steps`\n",
    "        jump_sign = np.sign(x_profile[window_steps])\n",
    "        if jump_sign == 0: jump_sign = 1\n",
    "        \n",
    "        windows.append(x_profile * jump_sign)\n",
    "        valid_indices.append(idx)\n",
    "        \n",
    "    X_windows = np.array(windows)\n",
    "    jumps_subset = jumps_df.loc[valid_indices].copy()\n",
    "    print(f\"  Extracted {len(X_windows)} valid windows.\")\n",
    "\n",
    "    # 4. Wavelet PCA\n",
    "    print(\"  Running Wavelet Kernel PCA...\")\n",
    "    wm = WaveletModel(n_layers=0, n_neurons=0, n_outputs=0, J=3, n_components=3, include_scattering_spectra=False)\n",
    "    embedding = wm.fit_transform(X_windows)\n",
    "    \n",
    "    d1 = embedding[:, 0]\n",
    "    \n",
    "    # Orient D1 (Reflexivity): Positive should correlate with Post-Jump Activity\n",
    "    center = window_steps\n",
    "    act_post = np.sum(np.abs(X_windows[:, center+1:]), axis=1)\n",
    "    act_pre = np.sum(np.abs(X_windows[:, :center]), axis=1)\n",
    "    asymmetry = (act_post - act_pre) / (act_post + act_pre + 1e-6)\n",
    "    \n",
    "    corr = np.corrcoef(d1, asymmetry)[0, 1]\n",
    "    if corr < 0:\n",
    "        print(f\"  Flipping D1 sign (correlation was {corr:.2f})\")\n",
    "        d1 *= -1\n",
    "        corr = -corr  # Update correlation after flipping\n",
    "        \n",
    "    jumps_subset[\"D1_reflexivity\"] = d1\n",
    "    jumps_subset[\"asymmetry\"] = asymmetry\n",
    "    print(f\"  D1-Asymmetry correlation: {corr:.3f}\")\n",
    "    \n",
    "    # Handcrafted Features\n",
    "    # D2 (Mean Reversion): Pre-Jump - Post-Jump\n",
    "    # D3 (Trend): Pre-Jump + Post-Jump\n",
    "    # t = +/- 1 step from center\n",
    "    jumps_subset[\"D2_mean_reversion\"] = X_windows[:, center - 1] - X_windows[:, center + 1]\n",
    "    jumps_subset[\"D3_trend\"] = X_windows[:, center - 1] + X_windows[:, center + 1]\n",
    "\n",
    "    # 5. Generate Plots\n",
    "    print(\"  Generating plots...\")\n",
    "    \n",
    "    # Scatter Plot: D1 vs Asymmetry (to verify separation)\n",
    "    slope, intercept, r_value, p_value, std_err = scipy_stats.linregress(\n",
    "        jumps_subset[\"asymmetry\"], jumps_subset[\"D1_reflexivity\"]\n",
    "    )\n",
    "    \n",
    "    fig_asym = px.scatter(\n",
    "        jumps_subset, x=\"asymmetry\", y=\"D1_reflexivity\", \n",
    "        color=\"asymmetry\",\n",
    "        title=f\"D1 (Reflexivity) vs Asymmetry ({subset_name}, N={len(X_windows)})<br>Correlation: {corr:.3f}, R²: {r_value**2:.3f}\",\n",
    "        color_continuous_scale=\"RdBu\", opacity=0.6,\n",
    "        hover_data=[\"ticker\", \"timestamp\"],\n",
    "        labels={\"asymmetry\": \"Asymmetry (Post-Pre)/(Post+Pre)\", \"D1_reflexivity\": \"D1 (Reflexivity)\"}\n",
    "    )\n",
    "    \n",
    "    # Add regression line\n",
    "    asym_range = np.linspace(jumps_subset[\"asymmetry\"].min(), jumps_subset[\"asymmetry\"].max(), 100)\n",
    "    reg_line = slope * asym_range + intercept\n",
    "    fig_asym.add_trace(go.Scatter(\n",
    "        x=asym_range, y=reg_line, mode='lines', name=f'Regression (R²={r_value**2:.3f})',\n",
    "        line=dict(color='red', width=2, dash='dash')\n",
    "    ))\n",
    "    \n",
    "    fig_asym.add_vline(x=0, line_dash=\"dash\", line_color=\"gray\", annotation_text=\"Symmetric\")\n",
    "    fig_asym.add_hline(y=0, line_dash=\"dash\", line_color=\"gray\")\n",
    "    \n",
    "    out_path_asym = os.path.join(output_dir, f\"{subset_name}_D1_asymmetry.html\")\n",
    "    fig_asym.write_html(out_path_asym)\n",
    "    print(f\"    Saved D1-Asymmetry plot to {out_path_asym}\")\n",
    "    save_plot(fig_asym, f\"reproduce_hong_kong_D1_asymmetry\", format='pdf')\n",
    "    fig_asym.show()\n",
    "    \n",
    "    # Scatter Plot: D1 vs D2 (Mean-Reversion) - Fig 5 equivalent\n",
    "    fig_mr = px.scatter(\n",
    "        jumps_subset, x=\"D1_reflexivity\", y=\"D2_mean_reversion\", color=\"D2_mean_reversion\",\n",
    "        title=f\"Reflexivity vs Mean-Reversion ({subset_name}, N={len(X_windows)})\", \n",
    "        color_continuous_scale=\"RdBu\", opacity=0.5,\n",
    "        hover_data=[\"ticker\", \"timestamp\"]\n",
    "    )\n",
    "    fig_mr.add_vline(x=0, line_dash=\"dash\"); fig_mr.add_hline(y=0, line_dash=\"dash\")\n",
    "    \n",
    "    out_path_scatter = os.path.join(output_dir, f\"{subset_name}_fig5_mr.html\")\n",
    "    fig_mr.write_html(out_path_scatter)\n",
    "    print(f\"    Saved scatter plot to {out_path_scatter}\")\n",
    "    save_plot(fig_mr, f\"reproduce_hong_kong_fig5_mr\", format='pdf')\n",
    "    fig_mr.show()\n",
    "    \n",
    "    # Scatter Plot: D1 vs D3 (Trend) - Fig 6 equivalent\n",
    "    fig_tr = px.scatter(\n",
    "        jumps_subset, x=\"D1_reflexivity\", y=\"D3_trend\", color=\"D3_trend\",\n",
    "        title=f\"Reflexivity vs Trend ({subset_name}, N={len(X_windows)})\", \n",
    "        color_continuous_scale=\"Viridis\", opacity=0.5,\n",
    "        hover_data=[\"ticker\", \"timestamp\"]\n",
    "    )\n",
    "    fig_tr.add_vline(x=0, line_dash=\"dash\"); fig_tr.add_hline(y=0, line_dash=\"dash\")\n",
    "    \n",
    "    out_path_scatter_tr = os.path.join(output_dir, f\"{subset_name}_fig6_tr.html\")\n",
    "    fig_tr.write_html(out_path_scatter_tr)\n",
    "    print(f\"    Saved scatter plot to {out_path_scatter_tr}\")\n",
    "    save_plot(fig_tr, f\"reproduce_hong_kong_fig6_tr\", format='pdf')\n",
    "    fig_tr.show()\n",
    "    \n",
    "    # Profile Plots (all three directions)\n",
    "    plot_profiles(X_windows, jumps_subset, output_dir, subset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29eba403",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Hong Kong data directory\n",
    "    data_dir = \"/home/janis/4A/timeseries/data/stooq/hongkong/5_min/hk/hkex_stocks\"\n",
    "    print(f\"Loading Hong Kong data from {data_dir}...\")\n",
    "    all_dfs = curate_stooq_dir_5min(data_dir, pattern=\"*.txt\", recursive=True)\n",
    "    \n",
    "    # Filter to stocks with enough data (e.g., > 500 points) to ensure meaningful jumps\n",
    "    valid_tickers = [t for t, d in all_dfs.items() if len(d) > 500]\n",
    "    print(f\"Found {len(valid_tickers)} valid tickers.\")\n",
    "    \n",
    "    if not valid_tickers:\n",
    "        print(\"No valid data found.\")\n",
    "        return\n",
    "    \n",
    "    # Sort by data length to pick the best ones\n",
    "    valid_tickers.sort(key=lambda t: len(all_dfs[t]), reverse=True)\n",
    "    \n",
    "    # Select Subsets\n",
    "    # 1. Small Subset: Top 5 most liquid/long stocks\n",
    "    tickers_5 = valid_tickers[:5] if len(valid_tickers) >= 5 else valid_tickers\n",
    "    dfs_5 = {t: all_dfs[t] for t in tickers_5}\n",
    "    \n",
    "    # 2. Large Subset: Top 30 (or all if less)\n",
    "    limit_30 = min(30, len(valid_tickers))\n",
    "    tickers_30 = valid_tickers[:limit_30]\n",
    "    dfs_30 = {t: all_dfs[t] for t in tickers_30}\n",
    "    \n",
    "    # Output Directory\n",
    "    try:\n",
    "        base_dir = os.path.dirname(__file__)\n",
    "    except NameError:\n",
    "        base_dir = os.getcwd()\n",
    "    output_dir = os.path.join(base_dir, \"hong_kong_outputs\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Run Comparisons\n",
    "    run_analysis_for_subset(dfs_5, \"5_Stocks_HongKong\", output_dir)\n",
    "    \n",
    "    if len(tickers_30) > len(tickers_5):\n",
    "        run_analysis_for_subset(dfs_30, f\"{len(tickers_30)}_Stocks_HongKong\", output_dir)\n",
    "    else:\n",
    "        print(\"Skipping large subset analysis (not enough stocks for distinct comparison).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f374e976",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b7d8d0",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
