\providecommand\numberofexercises{}
\XSIM{solution-body}{exercise-1=={\par \textbf {Important note:} This methodology is the result of approximately 15 hours of iterative development with many intermediate attempts. Crucially, \textbf {we use NO prior knowledge about DTMF frequencies and NO magic numbers}. The discovered frequency structure (4$\times $4 grid) emerges purely from unsupervised clustering on the data. \par \textbf {Code availability:} All code is fully reproducible at \href {https://github.com/janisaiad/timeseries}{github.com/janisaiad/timeseries} in the \texttt {refs/tps/assignment3/} directory with complete large-scale runs (100 training signals + 2 test signals) and all generated visualizations (600+ PNG files organized in \texttt {data/figures/signal\_i/} folders). The repository contains: (i) main pipeline script (\texttt {torun.py}), (ii) Bayesian regression (\texttt {bayesian\_regression.py}), (iii) test set processing (\texttt {process\_test\_set\_full\_plots.py}), (iv) all raw data (\texttt {X\_train.npy}, \texttt {X\_test.npy}, \texttt {y\_train.npy}, \texttt {y\_test.npy}), and (v) complete results (\texttt {data/} directory with JSON logs and figures). \par \textbf {Answer 1 - Methodology:} \par \textbf {Important design choice:} The signals contain complex noise including a chirp component of the form $A\sin (x \cdot (B - C\sin (x)))$ visible as a sinusoidal curve in the spectrogram. \textbf {We deliberately refused to manually remove this chirp} or apply any frequency-specific filtering, as this would require prior knowledge of noise structure. Instead, we rely entirely on unsupervised clustering to separate signal from noise automatically. \par \begin {itemize} \item \textbf {Step 1 - STFT:} We compute the Short-Time Fourier Transform with window=900 samples, 50\% overlap (fs=22.05 kHz). For each frequency bin $f_i$, we compute total energy by summing squared magnitudes across all time frames: $E(f_i) = \sum _t |Z_{i,t}|^2$. \par \item \textbf {Step 2 - Signal/Noise Separation (NO threshold, NO chirp removal):} We apply K-means with k=2 on the energy values $E(f_i)$. The cluster with higher center energy contains signal frequencies; the other contains noise (including the chirp). This \textbf {automatically separates} signal from noise without any manual threshold or frequency filtering. \par \item \textbf {Step 3 - Hierarchical Frequency Clustering (NO magic numbers):} Among high-energy bins, we filter to a wide DTMF band [400, 2000] Hz (conservative range, \textbf {no prior} on specific frequencies): \begin {enumerate} \item \textbf {Coarse split:} K-means (k=2) on 1D frequencies separates bins into two groups. The group with lower mean frequency is "low", the other is "high". \textbf {No threshold used} - clustering automatically determines the split point. \par \item \textbf {Fine clustering with elbow method:} Within each group (low and high), we apply the elbow method to determine the optimal number of clusters: \begin {itemize} \item We test k=1 to k=8 and compute inertia for each k \item We normalize both k and inertia to [0,1] \item We compute perpendicular distance from each point to the line connecting first and last points \item We select k with maximum distance (the "elbow") \item This typically yields k=4 for both low and high groups \end {itemize} The elbow method \textbf {automatically determines k from the data} - no manual selection of the number of frequency bands. \par \item Result: Typically 4 low-frequency clusters and 4 high-frequency clusters, forming a 4$\times $4 grid structure discovered purely from clustering. \end {enumerate} \par \textbf {Critical point:} We use clustering techniques (K-means + elbow) to \textbf {systematically avoid any threshold or magic number}. We have \textbf {NO prior knowledge} that DTMF uses frequencies between 500 Hz to 2000 Hz, or that there are exactly 4 low and 4 high frequencies, or what these frequencies are. The entire 4$\times $4 structure emerges automatically from unsupervised learning on the training data. \par \item \textbf {Step 4 - Changepoint Detection:} For each of the $\sim $8 frequency clusters, we select the frequency bin closest to the cluster center. We extract its energy time series and run PELT (Pruned Exact Linear Time) changepoint detection with BIC penalty $\text {pen} = 2\sigma ^2\log (T)$ where $\sigma $ is the standard deviation of the energy signal. This identifies temporal boundaries between DTMF symbols and silences. \par \item \textbf {Step 5 - Interval Ranking and Selection:} For each cluster, we build all intervals between consecutive changepoints. We compute mean energy in each interval. We aggregate all low-frequency intervals and all high-frequency intervals separately, rank by mean energy (descending), and select the top-N non-overlapping intervals where N is the ground truth sequence length. We then order selected intervals by start time. \par \item \textbf {Step 6 - Symbol Pairing and Bayesian Classification:} We pair the i-th low interval with the i-th high interval by temporal order, yielding frequency pairs $(f_{\text {low}}, f_{\text {high}})$. We train a Bayesian classifier where each symbol is modeled as a bivariate Gaussian on frequency pairs. Prediction uses maximum posterior: $\arg \max _s P(s | f_{\text {low}}, f_{\text {high}}) \propto P(s) \times \mathcal {N}(f_{\text {low}}; \mu _{\text {low}}^s, \sigma _{\text {low}}^s) \times \mathcal {N}(f_{\text {high}}; \mu _{\text {high}}^s, \sigma _{\text {high}}^s)$. \end {itemize} \par \textbf {Hyperparameter tuning:} Minimal. STFT window=900 found empirically (testing 768, 900, 1024). Changepoint detection uses \textbf {default BIC parameters} (lower penalties yielded no changepoints). All other parameters (k\_max=8 for elbow, [400,2000]Hz band, min\_duration=100 samples) are set conservatively without tuning. รง \textbf {Results:} Training on 100 signals yielded 540 frequency pairs with \textbf {100\% accuracy}. The unsupervised clustering discovered 4 low-frequency clusters (697.2, 761.4, 845.6, 931.0 Hz) and 4 high-frequency clusters (1211.3, 1325.8, 1479.3, 1617.5 Hz), forming a perfect 4$\times $4 DTMF grid with $<1\%$ error vs standard DTMF. All 16 symbols were identified without prior knowledge. \par \begin {figure}[h!] \centering \begin {minipage}[t]{0.32\textwidth } \centerline {\includegraphics [width=\textwidth ]{cluster1.png}} \centerline {\scriptsize Step 2: Signal/Noise (k=2)} \end {minipage} \hfill \begin {minipage}[t]{0.32\textwidth } \centerline {\includegraphics [width=\textwidth ]{cluster2.png}} \centerline {\scriptsize Step 3.1: Coarse split} \end {minipage} \hfill \begin {minipage}[t]{0.32\textwidth } \centerline {\includegraphics [width=\textwidth ]{cluster3.png}} \centerline {\scriptsize Elbow (low group)} \end {minipage} \vskip 0.5em \begin {minipage}[t]{0.32\textwidth } \centerline {\includegraphics [width=\textwidth ]{cluster4.png}} \centerline {\scriptsize Low/High clustering zoomed} \end {minipage} \hfill \begin {minipage}[t]{0.32\textwidth } \centerline {\includegraphics [width=\textwidth ]{cluster5.png}} \centerline {\scriptsize Low/High clustering } \end {minipage} \hfill \begin {minipage}[t]{0.32\textwidth } \centerline {\includegraphics [width=\textwidth ]{cluster6.png}} \centerline {\scriptsize Fine clustering} \end {minipage} \caption {Hierarchical clustering pipeline: (Step 2) K-means separates signal from noise, (Step 3.1) coarse k=2 split into low/high groups, (elbow) automatic k determination per group, (fine) final clusters forming 4$\times $4 structure} \label {fig:clustering-steps} \end {figure} \par \begin {figure}[h!] \centering \begin {minipage}[t]{0.48\textwidth } \centerline {\includegraphics [width=\textwidth ]{signal_66/01_stft_overview.png}} \centerline {\small (a) STFT and energy distributions} \end {minipage} \hfill \begin {minipage}[t]{0.48\textwidth } \centerline {\includegraphics [width=\textwidth ]{signal_66/04_energy_changepoints_per_cluster.png}} \centerline {\small (b) Changepoint detection per cluster} \end {minipage} \vskip 1em \begin {minipage}[t]{0.48\textwidth } \centerline {\includegraphics [width=\textwidth ]{signal_66/05_detected_symbols.png}} \centerline {\small (c) Detected symbols on signal} \end {minipage} \hfill \begin {minipage}[t]{0.48\textwidth } \centerline {\includegraphics [width=0.7\textwidth ]{dtmf_4x4_grid_heatmap.png}} \centerline {\scriptsize (d) Learned 4$\times $4 DTMF grid} \end {minipage} \caption {DTMF detection pipeline (example signal 66) showing discovered clusters} \label {fig:dtmf-pipeline} \end {figure} \par \begin {figure}[h!] \centering \begin {minipage}[t]{0.48\textwidth } \centerline {\includegraphics [width=\textwidth ]{frequency_grid_4x4.png}} \centerline {\small (a) Discovered 4$\times $4 frequency clusters} \end {minipage} \hfill \begin {minipage}[t]{0.48\textwidth } \centerline {\includegraphics [width=\textwidth ]{frequency_distributions_4clusters.png}} \centerline {\small (b) Frequency distributions showing 4 clusters} \end {minipage} \caption {Unsupervised clustering results: 4 low $\times $ 4 high frequency clusters discovered without prior knowledge} \label {fig:clusters} \end {figure} \par }||exercise-2=={Using the trained Bayesian model, we predict the test set sequences: \begin {itemize} \item Sequence 1: \texttt {88133} (5 symbols, 100\% confidence) \item Sequence 2: \texttt {11\#2\#\#} (6 symbols, 99.99\% confidence) \end {itemize} \par \textbf {Detailed predictions:} \begin {itemize} \item Test signal 0: Frequencies detected were (857.5Hz, 1323.0Hz)$\rightarrow $'8' (twice), (686.0Hz, 1200.5Hz)$\rightarrow $'1', (686.0Hz, 1470.0Hz)$\rightarrow $'3' (twice). \item Test signal 1: Frequencies detected were (686.0Hz, 1200.5Hz)$\rightarrow $'1' (twice), (931.0Hz, 1470.0Hz)$\rightarrow $'\#', (686.0Hz, 1323.0Hz)$\rightarrow $'2', (931.0Hz, 1470.0Hz)$\rightarrow $'\#' (twice). \end {itemize} \par The Bayesian classifier assigns each detected frequency pair $(f_{\text {low}}, f_{\text {high}})$ to the symbol with maximum posterior probability: $$P(\text {symbol} | f_{\text {low}}, f_{\text {high}}) \propto P(\text {symbol}) \times P(f_{\text {low}} | \text {symbol}) \times P(f_{\text {high}} | \text {symbol})$$ where priors $P(\text {symbol})$ are learned from training counts and likelihoods are Gaussian: $P(f | \text {symbol}) \sim \mathcal {N}(\mu , \sigma ^2)$ with parameters estimated from the 540 training frequency pairs.}||exercise-3=={\begin {figure} \centering \begin {minipage}[t]{0.32\textwidth } \centerline {\includegraphics [width=\textwidth ]{gm_all_m_R1.png}} \centerline {(a) $R=1$} \end {minipage} \hfill \begin {minipage}[t]{0.32\textwidth } \centerline {\includegraphics [width=\textwidth ]{gm_all_m_R3.png}} \centerline {(b) $R=3$} \end {minipage} \hfill \begin {minipage}[t]{0.32\textwidth } \centerline {\includegraphics [width=\textwidth ]{gm_all_m_R5.png}} \centerline {(c) $R=5$} \end {minipage} \caption {The SAGW kernels functions}\label {fig:sagw-kernels} \end {figure} We can see that as \(R\) increases, the support of the kernels grows (because \(R < M+1\)), and the oscillation frequency decreases. Each kernel spans exactly \(\pi \) radians, forming a single "mountain". Also, the kernels get more and more overlapped as \(R\) increases.}||exercise-4=={\par The stations with missing values are ARZAL, BATZ, BEG\_MEIL, BREST-GUIPAVAS, BRIGNOGAN, CAMARET, LANDIVISIAU, LANNAERO, LANVEOC, OUESSANT-STIFF, PLOUAY-SA, PLOUDALMEZEAU, PLOUGONVELIN, QUIMPER, RIEC SUR BELON, SIZUN, ST NAZAIRE-MONTOIR, VANNES-MEUCON. \par The threshold is equal 0.8291457286432161. We computed the maximum threshold and not the minimum since for low values of epsilon the condition will be surely met. \par The signal is least smooth at: 2014-01-21 03:00:00 \par The signal is the smoothest at: 2014-01-24 23:00:00}||exercise-5=={\begin {figure} \centering \begin {minipage}[t]{0.45\textwidth } \centerline {\includegraphics [width=\textwidth ]{frequency_least_smooth.png}} \centerline {(a) Least smooth signal} \end {minipage} \hfill \begin {minipage}[t]{0.45\textwidth } \centerline {\includegraphics [width=\textwidth ]{frequency_most_smooth.png}} \centerline {(b) Smoothest signal} \end {minipage} \vskip 1em \begin {minipage}[t]{0.45\textwidth } \centerline {\includegraphics [width=\textwidth ]{frequency_first_timestamp.png}} \centerline {(c) First available timestamp} \end {minipage} \caption {Classification of nodes into low/medium/high frequency}\label {fig:node-classif} \end {figure}}||exercise-6=={\begin {figure} \centering \begin {minipage}[t]{0.8\textwidth } \centerline {\includegraphics [width=\textwidth ]{average_temperature_majority_class.png}} \end {minipage} \caption {Average temperature. Markers' colours depend on the majority class.} \end {figure}}||exercise-7=={Let's assume we have \(T\) time stamps and the size of the graph \(G\) is \(n\). Let \(L_G\) be the laplacian of the initial graph, and let \(L_{G'}\) be the laplacian of the graph of size \(T\) that is a line, so it contains 1's only in \((i,i+1) \quad 1<=i<n\) and \((i,i-1)\quad 1<i<=n\) (symmetrically). Let \(I_N\) and \(I_T\) be identity matrix. Then the new laplacian can be expressed as: \[L_{H} = I_T \otimes L_G + \beta * L_{G'} \otimes I_N\] Where \(\beta \) is the weight we want to add in the edges representing a link with itself in consecutive timestamps.\\ It is a well known fact that if \(v_i\) and \(w_j\) are the set of eigenvectors of \(L_G\) and \(L_{G'}\) respectively, then \(w_j\otimes v_i\) are the set of eigenvectors of \(L_H\). If \(u_i\) and \(l_j\) are the eigenvalues of \(v_i\) and \(w_j\) respectively, then the eigenvalue of \(w_j\otimes v_i\) is \(u_i + \beta l_j\). \begin {figure} \centering \begin {minipage}[t]{0.8\textwidth } \centerline {\includegraphics [width=\textwidth ]{average_temperature_majority_class_timestamps.png}} \end {minipage} \caption {Average temperature. Markers' colours depend on the majority class.} \end {figure}}}
\XSIM{exercise-body}{exercise-1=={Design a procedure that takes a sound signal as input and outputs the sequence of symbols. To that end, you can use the provided training set. The signals have a varying number of symbols with a varying duration. There is a brief silence between each symbol. \par Describe in 5 to 10 lines your methodology and the calibration procedure (give the hyperparameter values). Hint: use the time-frequency representation of the signals, apply a change-point detection algorithm to find the starts and ends of the symbols and silences, and then classify each segment. \par }||exercise-2=={What are the two symbolic sequences encoded in the test set?}||exercise-3=={Plot the kernel functions $\hat {g}_m$ for $R=1$, $R=3$ and $R=5$ (take $\lambda _n=12$) on Figure~\ref {fig:sagw-kernels}. What is the influence of $R$?}||exercise-4=={Construct the graph using the distance matrix and exponential smoothing (use the median heuristics for the bandwidth parameter). \begin {itemize} \item Remove all stations with missing values in the temperature. \item Choose the minimum threshold so that the network is connected and the average degree is at least 3. \item What is the time where the signal is the least smooth? \item What is the time where the signal is the smoothest? \end {itemize}}||exercise-5=={(For the remainder, set $R=3$ for all wavelet transforms.) \par For each node $v$, the vector $[W_f(1, v), W_f(2, v),\dots , W_f(M, v)]$ can be used as a vector of features. We can for instance classify nodes into low/medium/high frequency: \begin {itemize} \item a node is considered low frequency if the scales $m\in \{1,2,3\}$ contain most of the energy, \item a node is considered medium frequency if the scales $m\in \{4,5,6\}$ contain most of the energy, \item a node is considered high frequency if the scales $m\in \{6,7,9\}$ contain most of the energy. \end {itemize} \par \par For both signals from the previous question (smoothest and least smooth) as well as the first available timestamp, apply this procedure and display on the map the result (one colour per class). \par }||exercise-6=={Display the average temperature and for each timestamp, adapt the marker colour to the majority class present in the graph.}||exercise-7=={The previous graph $G$ only uses spatial information. To take into account the temporal dynamic, we construct a larger graph $H$ as follows: a node is now \textit {a station at a particular time} and is connected to neighbouring stations (with respect to $G$) and to itself at the previous timestamp and the following timestamp. Notice that the new spatio-temporal graph $H$ is the Cartesian product of the spatial graph $G$ and the temporal graph $G'$ (which is simply a line graph, without loop). \par \begin {itemize} \item Express the Laplacian of $H$ using the Laplacian of $G$ and $G'$ (use Kronecker products). \item Express the eigenvalues and eigenvectors of the Laplacian of $H$ using the eigenvalues and eigenvectors of the Laplacian of $G$ and $G'$. \item Compute the wavelet transform of the temperature signal. \item Classify nodes into low/medium/high frequency and display the same figure as in the previous question. \end {itemize}}}
\XSIM{goal}{exercise}{points}{0}
\XSIM{totalgoal}{points}{0}
\XSIM{goal}{exercise}{bonus-points}{0}
\XSIM{totalgoal}{bonus-points}{0}
\XSIM{order}{1||2||3||4||5||6||7}
\XSIM{use}{}
\XSIM{use!}{}
\XSIM{used}{exercise-1=={true}||exercise-2=={true}||exercise-3=={true}||exercise-4=={true}||exercise-5=={true}||exercise-6=={true}||exercise-7=={true}}
\XSIM{print}{}
\XSIM{print!}{}
\XSIM{printed}{exercise-1=={true}||exercise-2=={true}||exercise-3=={true}||exercise-4=={true}||exercise-5=={true}||exercise-6=={true}||exercise-7=={true}}
\XSIM{total-number}{7}
\XSIM{exercise}{7}
\XSIM{types}{exercise}
\XSIM{idtypes}{1=={exercise}||2=={exercise}||3=={exercise}||4=={exercise}||5=={exercise}||6=={exercise}||7=={exercise}}
\XSIM{collections}{exercise-1=={all exercises}||exercise-2=={all exercises}||exercise-3=={all exercises}||exercise-4=={all exercises}||exercise-5=={all exercises}||exercise-6=={all exercises}||exercise-7=={all exercises}}
\XSIM{collection:all exercises}{exercise-1||exercise-2||exercise-3||exercise-4||exercise-5||exercise-6||exercise-7}
\setcounter{totalexerciseinall exercises}{7}
\XSIM{id}{exercise-1=={1}||exercise-2=={2}||exercise-3=={3}||exercise-4=={4}||exercise-5=={5}||exercise-6=={6}||exercise-7=={7}}
\XSIM{ID}{exercise-1=={1}||exercise-2=={2}||exercise-3=={3}||exercise-4=={4}||exercise-5=={5}||exercise-6=={6}||exercise-7=={7}}
\XSIM{counter}{exercise-1=={1}||exercise-2=={2}||exercise-3=={3}||exercise-4=={4}||exercise-5=={5}||exercise-6=={6}||exercise-7=={7}}
\XSIM{counter-value}{exercise-1=={1}||exercise-2=={2}||exercise-3=={3}||exercise-4=={4}||exercise-5=={5}||exercise-6=={6}||exercise-7=={7}}
\XSIM{solution}{}
\XSIM{section-value}{exercise-1=={2}||exercise-2=={2}||exercise-3=={3}||exercise-4=={3}||exercise-5=={3}||exercise-6=={3}||exercise-7=={3}}
\XSIM{section}{exercise-1=={2}||exercise-2=={2}||exercise-3=={3}||exercise-4=={3}||exercise-5=={3}||exercise-6=={3}||exercise-7=={3}}
\XSIM{sectioning}{exercise-1=={{0}{2}{0}{0}{0}}||exercise-2=={{0}{2}{0}{0}{0}}||exercise-3=={{0}{3}{0}{0}{0}}||exercise-4=={{0}{3}{0}{0}{0}}||exercise-5=={{0}{3}{0}{0}{0}}||exercise-6=={{0}{3}{0}{0}{0}}||exercise-7=={{0}{3}{0}{0}{0}}}
\XSIM{subtitle}{}
\XSIM{points}{}
\XSIM{bonus-points}{}
\XSIM{page-value}{exercise-1=={2}||exercise-2=={4}||exercise-3=={7}||exercise-4=={8}||exercise-5=={9}||exercise-6=={11}||exercise-7=={12}}
\XSIM{page}{exercise-1=={2}||exercise-2=={4}||exercise-3=={7}||exercise-4=={8}||exercise-5=={9}||exercise-6=={11}||exercise-7=={12}}
\XSIM{tags}{}
\XSIM{topics}{}
\XSIM{userpoints}{}
\XSIM{bodypoints}{}
\XSIM{userbonus-points}{}
\XSIM{bodybonus-points}{}
