\providecommand\numberofexercises{}
\XSIM{solution-body}{exercise-1=={For i.i.d.\ random variables with finite variance, the variance of the sample mean is \[ \operatorname {Var}(\bar {Y}_n) = \frac {\sigma ^2}{n} \] and the convergence rate is $1/\sqrt {n}$. \par For a stationary process $\{Y_t\}$ with $\sum _k |\gamma (k)| < +\infty $, \[ \operatorname {Var}(\bar {Y}_n) = \frac {1}{n^2} \sum _{j=1}^n \sum _{k=1}^n \operatorname {Cov}(Y_j, Y_k) = \frac {1}{n^2} \sum _{j=1}^n \sum _{k=1}^n \gamma (j-k) \] which can also be written as \[ \operatorname {Var}(\bar {Y}_n) = \frac {1}{n} \sum _{h=-(n-1)}^{n-1} \left (1 - \frac {|h|}{n}\right ) \gamma (h) \] If $\sum _k |\gamma (k)| < +\infty $, then as $n \to \infty $, \[ \operatorname {Var}(\bar {Y}_n) \to \frac {1}{n} \sum _{h=-\infty }^{\infty } \gamma (h) \] so $\operatorname {Var}(\bar {Y}_n) = O(1/n)$, i.e., the convergence rate is $1/\sqrt {n}$ like in the i.i.d.\ case. \par }||exercise-2=={First, since $Y_t$ is a linear combination of zero mean white noises, we have \[ \mathbb {E}[Y_t] = \sum _{k=0}^\infty \psi _k \mathbb {E}[\varepsilon _{t-k}] = 0. \] \par Now for the covariance: \[ \mathbb {E}[Y_tY_{t-h}] = \mathbb {E}\left [ \left ( \sum _{k=0}^\infty \psi _k \varepsilon _{t-k} \right ) \left ( \sum _{l=0}^\infty \psi _l \varepsilon _{t-h-l} \right ) \right ] = \sum _{k=0}^\infty \sum _{l=0}^\infty \psi _k \psi _l\, \mathbb {E}[\varepsilon _{t-k}\varepsilon _{t-h-l}] \] Since $\{\varepsilon _t\}$ are uncorrelated (and independent), the only nonzero terms are when $t-k = t-h-l$, i.e., $k = h + l$. Therefore, \[ \mathbb {E}[Y_t Y_{t-h}] = \sum _{l=0}^\infty \psi _{h+l} \psi _l\, \mathbb {E}[\varepsilon _{t-(h+l)}^2] = \sigma _\varepsilon ^2 \sum _{l=0}^\infty \psi _{h+l}\psi _l \] So the covariance at lag $h$ is \[ \gamma (h) = \mathrm {Cov}(Y_t, Y_{t-h}) = \sigma _\varepsilon ^2 \sum _{l=0}^\infty \psi _{h+l} \psi _l \] which does not depend on $t$: the process is weakly stationary. \par To compute the power spectrum, recall that for a stationary process, \[ S(f) = \sum _{k=-\infty }^{+\infty } \gamma (k) e^{-2\pi i f k}. \] Now, let's expand the modulus square in $S(f) = \sigma _\varepsilon ^2 |\phi (e^{-2\pi i f})|^2$: \[ \phi (e^{-2\pi i f}) = \sum _{k=0}^\infty \psi _k e^{-2\pi i f k} \] so \[ |\phi (e^{-2\pi i f})|^2 = \phi (e^{-2\pi i f})\, \overline {\phi (e^{-2\pi i f})} = \left ( \sum _{k=0}^\infty \psi _k e^{-2\pi i f k} \right ) \left ( \sum _{j=0}^\infty \psi _j e^{2\pi i f j} \right ) \] \[ = \sum _{k=0}^\infty \sum _{j=0}^\infty \psi _k \psi _j e^{-2\pi i f (k-j)} \] Therefore, the power spectrum is \[ S(f) = \sigma _\varepsilon ^2 \sum _{k=0}^\infty \sum _{j=0}^\infty \psi _k \psi _j\, e^{-2\pi i f (k-j)} \] which is an explicit expansion of the complex conjugate product. \par }||exercise-3=={First, let us establish the stationarity and the form of the solution. Let $\nu _1 = 1/r_1$ and $\nu _2 = 1/r_2$. Since $|r_i| > 1$, we have $|\nu _i| < 1$. Using the state-space representation, let $X_t = [Y_t, Y_{t-1}]^\top $. We can write: \begin {equation} X_t = \Phi X_{t-1} + \xi _t, \quad \text {with} \quad \Phi = \begin {pmatrix} \phi _1 & \phi _2 \\ 1 & 0 \end {pmatrix}, \quad \xi _t = \begin {pmatrix} \varepsilon _t \\ 0 \end {pmatrix}. \end {equation} By iterating this equation, we obtain $X_t = \sum _{k=0}^{\infty } \Phi ^k \xi _{t-k}$. Since the eigenvalues of $\Phi $ are exactly $\nu _1$ and $\nu _2$ (roots of $z^2 - \phi _1 z - \phi _2 = 0$), and $|\nu _i| < 1$, the spectral radius $\rho (\Phi ) < 1$. Thus, the series converges in $L^2$, defined as a sum of independent Gaussian variables. The variance of the process converges to a finite limit $\gamma (0)$, confirming stationarity. \par We now derive $\gamma (0)$ and $\gamma (1)$ algebraically using the Yule-Walker equations. Multiplying the AR(2) equation by $Y_{t-\tau }$ and taking expectations yields the recursion: \begin {equation} \gamma (\tau ) = \phi _1 \gamma (\tau -1) + \phi _2 \gamma (\tau -2), \quad \forall \tau \geq 1. \end {equation} For $\tau =0$, taking the expectation with $Y_t$ (noting $E[\varepsilon _t Y_t] = \sigma ^2$): \begin {equation} \label {eq:yw0} \gamma (0) = \phi _1 \gamma (1) + \phi _2 \gamma (2) + \sigma ^2. \end {equation} For $\tau =1$: \begin {equation} \label {eq:yw1} \gamma (1) = \phi _1 \gamma (0) + \phi _2 \gamma (1) \implies \gamma (1)(1-\phi _2) = \phi _1 \gamma (0) \implies \gamma (1) = \frac {\phi _1}{1-\phi _2}\gamma (0). \end {equation} Substituting $\gamma (2) = \phi _1 \gamma (1) + \phi _2 \gamma (0)$ into \eqref {eq:yw0}: \begin {equation} \gamma (0) = \phi _1 \gamma (1) + \phi _2 (\phi _1 \gamma (1) + \phi _2 \gamma (0)) + \sigma ^2 = (\phi _1^2 + \phi _2)\gamma (1) + \phi _2^2 \gamma (0) + \sigma ^2. \end {equation} Substituting $\gamma (1)$ from \eqref {eq:yw1}: \begin {equation} \gamma (0) (1 - \phi _2^2) - \gamma (0) \frac {\phi _1 (\phi _1 + \phi _1 \phi _2)}{1-\phi _2} = \sigma ^2. \end {equation} Factorizing and solving for $\gamma (0)$: \begin {equation} \gamma (0) \left [ \frac {(1-\phi _2)(1-\phi _2^2) - \phi _1^2(1+\phi _2)}{1-\phi _2} \right ] = \sigma ^2 \implies \gamma (0) = \frac {(1-\phi _2)\sigma ^2}{(1+\phi _2)((1-\phi _2)^2 - \phi _1^2)}. \end {equation} Using Vieta's formulas, $\phi _1 = \nu _1 + \nu _2$ and $\phi _2 = -\nu _1 \nu _2$. The denominator term $(1-\phi _2)^2 - \phi _1^2$ simplifies to $(1+\nu _1\nu _2)^2 - (\nu _1+\nu _2)^2 = (1-\nu _1^2)(1-\nu _2^2)$. Thus: \begin {equation} \gamma (0) = \frac {(1+\nu _1 \nu _2) \sigma ^2}{(1-\nu _1 \nu _2)(1-\nu _1^2)(1-\nu _2^2)}. \end {equation} \par The autocovariance satisfies the homogeneous difference equation for $\tau \geq 1$, so its solution is of the form $\gamma (\tau ) = c_1 \nu _1^{\tau } + c_2 \nu _2^{\tau }$. We solve for $c_1, c_2$ using the boundary conditions at $\tau =0, 1$: \begin {equation} \begin {pmatrix} 1 & 1 \\ \nu _1 & \nu _2 \end {pmatrix} \begin {pmatrix} c_1 \\ c_2 \end {pmatrix} = \begin {pmatrix} \gamma (0) \\ \gamma (1) \end {pmatrix}. \end {equation} Inverting the matrix: \begin {equation} \begin {pmatrix} c_1 \\ c_2 \end {pmatrix} = \frac {1}{\nu _2 - \nu _1} \begin {pmatrix} \nu _2 & -1 \\ -\nu _1 & 1 \end {pmatrix} \begin {pmatrix} \gamma (0) \\ \gamma (1) \end {pmatrix}. \end {equation} Using $\gamma (1) = \frac {\nu _1+\nu _2}{1+\nu _1\nu _2}\gamma (0)$, we calculate $c_1$: \begin {equation} c_1 = \frac {\nu _2 \gamma (0) - \gamma (1)}{\nu _2 - \nu _1} = \frac {\gamma (0)}{\nu _2 - \nu _1} \left ( \nu _2 - \frac {\nu _1+\nu _2}{1+\nu _1\nu _2} \right ) = \frac {\gamma (0)}{\nu _2 - \nu _1} \frac {\nu _2 + \nu _1\nu _2^2 - \nu _1 - \nu _2}{1+\nu _1\nu _2} = \gamma (0) \frac {\nu _1\nu _2^2 - \nu _1}{(\nu _2-\nu _1)(1+\nu _1\nu _2)}. \end {equation} This simplifies to: \begin {equation} c_1 = \frac {\sigma ^2 \nu _1 (1-\nu _2^2)}{(\nu _1-\nu _2)(1-\nu _1\nu _2)(1-\nu _1^2)(1-\nu _2^2)} = \frac {\sigma ^2 \nu _1}{(\nu _1-\nu _2)(1-\nu _1\nu _2)(1-\nu _1^2)}. \end {equation} By symmetry for $c_2$ and expressing in terms of roots $r_i = 1/\nu _i$: \begin {equation} \gamma (\tau ) = \sigma ^2 \sum _{i=1}^2 \frac {r_i^{-|\tau |}}{(1-r_i^{-2})(1-r_1^{-1}r_2^{-1})(r_i^{-1}-r_{3-i}^{-1})} r_i^{-1}. \end {equation} \par \textbf {The left correlogram exhibits oscillating behavior characteristic of complex conjugate roots in the characteristic polynomial, whereas the right correlogram displays a monotonic exponential decay typical of real roots, allowing us to identify the left as the complex case and the right as the real case.} \par \par \par The power spectrum density $S(f)$ is the Fourier transform of the autocovariance function: \begin {equation} S(f) = \sum _{\tau =-\infty }^{\infty } \gamma (\tau ) e^{-i 2\pi f \tau }. \end {equation} Using the expression derived previously: \begin {equation} \gamma (\tau ) = c_1 \nu _1^{|\tau |} + c_2 \nu _2^{|\tau |}, \quad \text {where } \nu _i = 1/r_i. \end {equation} We can split the sum for each component $k \in \{1, 2\}$: \begin {equation} S_k(f) = \sum _{\tau =-\infty }^{\infty } \nu _k^{|\tau |} e^{-i 2\pi f \tau } = 1 + \sum _{\tau =1}^{\infty } (\nu _k e^{-i 2\pi f})^\tau + \sum _{\tau =1}^{\infty } (\nu _k e^{i 2\pi f})^\tau . \end {equation} Since $|\nu _k| < 1$, these are convergent geometric series: \begin {equation} S_k(f) = 1 + \frac {\nu _k e^{-i 2\pi f}}{1 - \nu _k e^{-i 2\pi f}} + \frac {\nu _k e^{i 2\pi f}}{1 - \nu _k e^{i 2\pi f}} = 1 + \frac {\nu _k e^{-i \omega }}{1 - \nu _k e^{-i \omega }} + \frac {\nu _k e^{i \omega }}{1 - \nu _k e^{i \omega }}, \end {equation} where $\omega = 2\pi f$. Combining terms: \begin {equation} S_k(f) = \frac {(1 - \nu _k e^{-i \omega })(1 - \nu _k e^{i \omega }) + \nu _k e^{-i \omega }(1 - \nu _k e^{i \omega }) + \nu _k e^{i \omega }(1 - \nu _k e^{-i \omega })}{|1 - \nu _k e^{-i \omega }|^2}. \end {equation} The numerator simplifies to: \begin {equation} 1 - \nu _k e^{i \omega } - \nu _k e^{-i \omega } + \nu _k^2 + \nu _k e^{-i \omega } - \nu _k^2 + \nu _k e^{i \omega } - \nu _k^2 = 1 - \nu _k^2. \end {equation} Thus: \begin {equation} S_k(f) = \frac {1 - \nu _k^2}{|1 - \nu _k e^{-i 2\pi f}|^2}. \end {equation} Substituting $c_k$: \begin {equation} S(f) = c_1 \frac {1 - \nu _1^2}{|1 - \nu _1 z^{-1}|^2} + c_2 \frac {1 - \nu _2^2}{|1 - \nu _2 z^{-1}|^2}, \quad \text {with } z = e^{i 2\pi f}. \end {equation} Recall from the derivation of $\gamma (0)$: \begin {equation} c_1 (1-\nu _1^2) = \frac {\sigma ^2 \nu _1}{(\nu _1 - \nu _2)(1 - \nu _1 \nu _2)}, \quad c_2 (1-\nu _2^2) = \frac {\sigma ^2 \nu _2}{(\nu _2 - \nu _1)(1 - \nu _1 \nu _2)}. \end {equation} So: \begin {equation} S(f) = \frac {\sigma ^2}{1 - \nu _1 \nu _2} \left ( \frac {\nu _1}{(\nu _1 - \nu _2)|1 - \nu _1 z^{-1}|^2} + \frac {\nu _2}{(\nu _2 - \nu _1)|1 - \nu _2 z^{-1}|^2} \right ). \end {equation} Finding a common denominator $D = |1 - \nu _1 z^{-1}|^2 |1 - \nu _2 z^{-1}|^2 = |(1 - \nu _1 z^{-1})(1 - \nu _2 z^{-1})|^2 = |1 - (\nu _1+\nu _2)z^{-1} + \nu _1\nu _2 z^{-2}|^2$. This matches $|\phi (e^{-i 2\pi f})|^2$ since $\phi _1 = \nu _1+\nu _2$ and $\phi _2 = -\nu _1\nu _2$. \par The numerator term is: \begin {equation} N = \frac {\sigma ^2}{(\nu _1 - \nu _2)(1 - \nu _1 \nu _2)} \left [ \nu _1 (1 - \nu _2 z)(1 - \nu _2 z^{-1}) - \nu _2 (1 - \nu _1 z)(1 - \nu _1 z^{-1}) \right ]. \end {equation} Expanding the bracket: \begin {align} & \nu _1 (1 - \nu _2 (z + z^{-1}) + \nu _2^2) - \nu _2 (1 - \nu _1 (z + z^{-1}) + \nu _1^2) \\ &= \nu _1 - \nu _1 \nu _2 (z + z^{-1}) + \nu _1 \nu _2^2 - \nu _2 + \nu _1 \nu _2 (z + z^{-1}) - \nu _1^2 \nu _2 \\ &= (\nu _1 - \nu _2) + \nu _1 \nu _2 (\nu _2 - \nu _1) = (\nu _1 - \nu _2)(1 - \nu _1 \nu _2). \end {align} Thus, the entire numerator $N$ simplifies to just $\sigma ^2$. \begin {equation} S(f) = \frac {\sigma ^2}{|\phi (e^{-i 2\pi f})|^2} = \frac {\sigma ^2}{|1 - \phi _1 e^{-i 2\pi f} - \phi _2 e^{-i 4\pi f}|^2}. \end {equation} \par To analyze the peak of the spectrum, let us factorize the characteristic polynomial as $\phi (z) = (1 - z/r_1)(1 - z/r_2)$. Assuming complex conjugate roots $r_{1,2} = \rho e^{\pm i \theta }$ with $\rho > 1$, the power spectrum is given by: \begin {equation} S(f) = \frac {\sigma ^2}{|1 - \rho ^{-1}e^{i(2\pi f - \theta )}|^2 |1 - \rho ^{-1}e^{i(2\pi f + \theta )}|^2}. \end {equation} T For complex conjugate roots $r_{1,2} = r e^{\pm i\theta }$ with $r = 1.05$ and $\theta = 2\pi /6$, we have $\nu _{1,2} = r^{-1} e^{\pm i\theta }$ where $r^{-1} \approx 0.952$. The power spectrum is: \begin {equation} S(f) = \frac {\sigma ^2}{|1 - \phi _1 e^{-i 2\pi f} - \phi _2 e^{-i 4\pi f}|^2} = \frac {\sigma ^2}{|(1 - \nu _1 e^{-i 2\pi f})(1 - \nu _2 e^{-i 2\pi f})|^2}. \end {equation} For frequency $f = f_0 := \theta /(2\pi ) = 1/6$ Hz, we have $e^{-i 2\pi f_0} = e^{-i\theta }$. Thus: \begin {equation} |1 - \nu _1 e^{-i\theta }|^2 = |1 - r^{-1} e^{i\theta } e^{-i\theta }|^2 = |1 - r^{-1}|^2 = (1 - r^{-1})^2. \end {equation} Similarly, $|1 - \nu _2 e^{-i\theta }|^2 = |1 - r^{-1} e^{-i\theta } e^{-i\theta }|^2 = |1 - r^{-1} e^{-i 2\theta }|^2$. Since $r \approx 1$, the denominator $|1 - r^{-1}|^2 = (r - 1)^2/r^2 \approx (r-1)^2$ is very small. For $r = 1.05$, we have $(r-1)^2 = 0.05^2 = 0.0025$, yielding: \begin {equation} S(f_0) \approx \frac {\sigma ^2}{(r-1)^2 \cdot |1 - r^{-1} e^{-i 2\theta }|^2} \approx \frac {\sigma ^2}{0.0025 \cdot C} \sim 400 \sigma ^2 / C, \end {equation} where $C = |1 - r^{-1} e^{-i 2\theta }|^2 \approx O(1)$. This produces a sharp spectral peak at frequency $f = \theta /(2\pi ) = 1/6$ Hz, corresponding to the imaginary part (phase) of the complex roots, with amplitude scaling as $(r-1)^{-2}$, explaining the observed peak around magnitude $\sim 350$ in the periodogram. \par We can see that the process is stationnary non diverging and the fourier spectrum shows a peak at the frequency of the roots.}||exercise-4=={\par \par \begin {figure} \centering \begin {minipage}[t]{0.45\textwidth } \centerline {\includegraphics [width=\textwidth ]{example-image-golden}} \centerline {Norms of the successive residuals} \end {minipage} \hfill \begin {minipage}[t]{0.45\textwidth } \centerline {\includegraphics [width=\textwidth ]{example-image-golden}} \centerline {Reconstruction with 10 atoms} \end {minipage} \caption {Question 4} \end {figure} \par \par \par }}
\XSIM{exercise-body}{exercise-1=={An estimator $\hat {\theta }_n$ is consistent if it converges in probability when the number $n$ of samples grows to $\infty $ to the true value $\theta \in \mathbb {R}$ of a parameter, i.e. $\hat {\theta }_n \xrightarrow {\mathcal {D}} \theta $. \par \begin {itemize} \item Recall the rate of convergence of the sample mean for i.i.d.\ random variables with finite variance. \item Let $\{Y_t\}_{t\geq 1}$ a wide-sense stationary process such that $\sum _k |\gamma (k)| < +\infty $. Show that the sample mean $\bar {Y}_n = (Y_1+\dots +Y_n)/n$ is consistent and enjoys the same rate of convergence as the i.i.d.\ case. (Hint: bound $\mathbb {E}[(\bar {Y}_n-\mu )^2]$ with the $\gamma (k)$ and recall that convergence in $L_2$ implies convergence in probability.) \end {itemize} \par }||exercise-2=={Let $\{Y_t\}_{t\geq 0}$ be a random process defined by \begin {equation}\label {eq:ma-inf} Y_t = \varepsilon _t + \psi _1 \varepsilon _{t-1} + \psi _2 \varepsilon _{t-2} + \dots = \sum _{k=0}^{\infty } \psi _k\varepsilon _{t-k} \end {equation} where $(\psi _k)_{k\geq 0} \subset \mathbb {R}$ ($\psi _k=1$) are square summable, \ie $\sum _k \psi _k^2 < \infty $ and $\{\varepsilon _t\}_t$ is a zero mean white noise of variance $\sigma _\varepsilon ^2$. (Here, the infinite sum of random variables is the limit in $L_2$ of the partial sums.) \begin {itemize} \item Derive $\mathbb {E}(Y_t)$ and $\mathbb {E}(Y_t Y_{t-k})$. Is this process weakly stationary? \item Show that the power spectrum of $\{Y_t\}_{t}$ is $S(f) = \sigma _\varepsilon ^2 |\phi (e^{-2\pi \iu f})|^2$ where $\phi (z) = \sum _j \psi _j z^j$. (Assume a sampling frequency of 1 Hz.) \end {itemize} \par The process $\{Y_t\}_{t}$ is a moving average of infinite order. Wold's theorem states that any weakly stationary process can be written as the sum of the deterministic process and a stochastic process which has the form~\eqref {eq:ma-inf}. \par }||exercise-3=={Let $\{Y_t\}_{t\geq 1}$ be an AR(2) process, i.e. \begin {equation} Y_t = \phi _1 Y_{t-1} + \phi _2 Y_{t-2} + \varepsilon _t \end {equation} with $\phi _1, \phi _2\in \mathbb {R}$. The associated characteristic polynomial is $\phi (z):=1-\phi _1 z - \phi _2 z^2$. Assume that $\phi $ has two distinct roots (possibly complex) $r_1$ and $r_2$ such that $|r_i|>1$. Properties on the roots of this polynomial drive the behavior of this process. \par \par \begin {itemize} \item Express the autocovariance coefficients $\gamma (\tau )$ using the roots $r_1$ and $r_2$. \item Figure~\ref {fig:q-ar-2-corr} shows the correlograms of two different AR(2) processes. Can you tell which one has complex roots and which one has real roots? \item Express the power spectrum $S(f)$ (assume the sampling frequency is 1 Hz) using $\phi (\cdot )$. \item Choose $\phi _1$ and $\phi _2$ such that the characteristic polynomial has two complex conjugate roots of norm $r=1.05$ and phase $\theta =2\pi /6$. Simulate the process $\{Y_t\}_t$ (with $n=2000$) and display the signal and the periodogram (use a smooth estimator) on Figure~\ref {fig:q-ar-2}. What do you observe? \end {itemize} \par \par \begin {figure} \centering \begin {minipage}[t]{0.45\textwidth } \centerline {\includegraphics [width=\textwidth ]{images/acf1.pdf}} \centerline {Correlogram of the first AR(2)} \end {minipage} \hfill \begin {minipage}[t]{0.45\textwidth } \centerline {\includegraphics [width=\textwidth ]{images/acf2.pdf}} \centerline {Correlogram of the second AR(2)} \end {minipage} \caption {Two AR(2) processes}\label {fig:q-ar-2-corr} \end {figure} \par \par \par }||exercise-4=={For the signal provided in the notebook, learn a sparse representation with MDCT atoms. The dictionary is defined as the concatenation of all shifted MDCDT atoms for scales $L$ in $[32, 64, 128, 256, 512, 1024]$. \par \begin {itemize} \item For the sparse coding, implement the Orthogonal Matching Pursuit (OMP). (Use convolutions to compute the correlation coefficients.) \item Display the norm of the successive residuals and the reconstructed signal with 10 atoms. \end {itemize} \par }}
\XSIM{goal}{exercise}{points}{0}
\XSIM{totalgoal}{points}{0}
\XSIM{goal}{exercise}{bonus-points}{0}
\XSIM{totalgoal}{bonus-points}{0}
\XSIM{order}{1||2||3||4}
\XSIM{use}{}
\XSIM{use!}{}
\XSIM{used}{exercise-1=={true}||exercise-2=={true}||exercise-3=={true}||exercise-4=={true}}
\XSIM{print}{}
\XSIM{print!}{}
\XSIM{printed}{exercise-1=={true}||exercise-2=={true}||exercise-3=={true}||exercise-4=={true}}
\XSIM{total-number}{4}
\XSIM{exercise}{4}
\XSIM{types}{exercise}
\XSIM{idtypes}{1=={exercise}||2=={exercise}||3=={exercise}||4=={exercise}}
\XSIM{collections}{exercise-1=={all exercises}||exercise-2=={all exercises}||exercise-3=={all exercises}||exercise-4=={all exercises}}
\XSIM{collection:all exercises}{exercise-1||exercise-2||exercise-3||exercise-4}
\setcounter{totalexerciseinall exercises}{4}
\XSIM{id}{exercise-1=={1}||exercise-2=={2}||exercise-3=={3}||exercise-4=={4}}
\XSIM{ID}{exercise-1=={1}||exercise-2=={2}||exercise-3=={3}||exercise-4=={4}}
\XSIM{counter}{exercise-1=={1}||exercise-2=={2}||exercise-3=={3}||exercise-4=={4}}
\XSIM{counter-value}{exercise-1=={1}||exercise-2=={2}||exercise-3=={3}||exercise-4=={4}}
\XSIM{solution}{}
\XSIM{section-value}{exercise-1=={2}||exercise-2=={3}||exercise-3=={3}||exercise-4=={4}}
\XSIM{section}{exercise-1=={2}||exercise-2=={3}||exercise-3=={3}||exercise-4=={4}}
\XSIM{sectioning}{exercise-1=={{0}{2}{0}{0}{0}}||exercise-2=={{0}{3}{0}{0}{0}}||exercise-3=={{0}{3}{0}{0}{0}}||exercise-4=={{0}{4}{0}{0}{0}}}
\XSIM{subtitle}{exercise-2=={Infinite order moving average MA($\infty $)}||exercise-3=={AR(2) process}||exercise-4=={Sparse coding with OMP}}
\XSIM{points}{}
\XSIM{bonus-points}{}
\XSIM{page-value}{exercise-1=={1}||exercise-2=={3}||exercise-3=={5}||exercise-4=={9}}
\XSIM{page}{exercise-1=={1}||exercise-2=={3}||exercise-3=={5}||exercise-4=={9}}
\XSIM{tags}{}
\XSIM{topics}{}
\XSIM{userpoints}{}
\XSIM{bodypoints}{}
\XSIM{userbonus-points}{}
\XSIM{bodybonus-points}{}
